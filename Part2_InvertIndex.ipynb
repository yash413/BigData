{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfwKUF80nX6L",
        "outputId": "84a6eea8-a6fd-4682-b2b0-7f6589637911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=6fb7bda88787c249f469bd30d2dd2b290a2a242924eb96473f0bda8681134420\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import log, col\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "318ewtCBnbo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext()\n",
        "spark = SparkSession.builder.appName(\"TF-IDF\").getOrCreate()"
      ],
      "metadata": {
        "id": "wNys27plnyrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load input file\n",
        "lines_rdd = sc.textFile(\"/content/userdata.txt\").zipWithIndex().map(lambda x: (x[1], x[0]))"
      ],
      "metadata": {
        "id": "7hS7EoOun3dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map function\n",
        "def map_func(line):\n",
        "    if isinstance(line, tuple):\n",
        "        line_text = line[1]\n",
        "        tokens = line_text.strip().split(\",\")\n",
        "        results = []\n",
        "        for token in tokens:\n",
        "            eachtoken = token.split(\" \")\n",
        "            for i in eachtoken:\n",
        "                if i.strip():\n",
        "                    results.append((i.lower(), line[0]))\n",
        "        return results\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "\n",
        "# reduce function\n",
        "def reduce_func(values):\n",
        "    line_counts = {}\n",
        "    for line_number in values:\n",
        "        line_counts[line_number] = line_counts.get(line_number, 0) + 1\n",
        "    sorted_counts = sorted(line_counts.items())\n",
        "    return sorted_counts"
      ],
      "metadata": {
        "id": "ib0-_9VUozTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the map-reduce transformation to create the inverted index\n",
        "inverted_index_rdd = lines_rdd.flatMap(map_func).groupByKey().mapValues(reduce_func)"
      ],
      "metadata": {
        "id": "QRtrkHj2pFOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each word\n",
        "word_counts_rdd = inverted_index_rdd.map(lambda x: (x[0], len(x[1])))"
      ],
      "metadata": {
        "id": "g3HzPFu4pSt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_count = word_counts_rdd.map(lambda x: x[1]).max()\n",
        "max_words_rdd = word_counts_rdd.filter(lambda x: x[1] == max_count).map(lambda x: x[0])"
      ],
      "metadata": {
        "id": "3pQzTvSOpWK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_words_list = max_words_rdd.collect()"
      ],
      "metadata": {
        "id": "392fZIY6pZ2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the output in the required format\n",
        "for word in max_words_list:\n",
        "    print(f\"{word}\\t{max_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aonN4RBNpdj0",
        "outputId": "87fcd114-1828-4f39-f7a6-9ea0347fa647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "us\t49995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8CO92sMspggk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}